{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c91d93-14ad-4ce8-b3da-454c16e66889",
   "metadata": {},
   "source": [
    "# Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63b1938-daaf-483d-b86c-9bb60361ce91",
   "metadata": {},
   "source": [
    "In this homework assignment you will train and evaluate a toy speech recognizer that can recognize 'yes' and 'no'.  As we move into the latter half of the course, note that the assignments will have less and less hand holding.  The goal is for you to become more and more independent, so that you are ready to work independently on the final project.  This assignment should be done in pairs and only one submission needs to be submitted on Gradescope for each team.  In your submission, you should create a single .zip file that contains your jupyter notebook, audio data, annotation data, and generated prediction files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b40e47-d050-46db-9870-84796af00cca",
   "metadata": {},
   "source": [
    "Please indicate the team member names here: ________________________\n",
    "\n",
    "How many hours you each spent on this assignment: _________ (partner 1), __________ (partner 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc250c8-be3d-4ee3-903b-041c64d7c14c",
   "metadata": {},
   "source": [
    "This assignment will be broken down into 5 main sections:\n",
    "1. Collect data & annotate (10 points)\n",
    "2. Train model using manual annotations (20 points)\n",
    "3. Perform inference on test data (20 points)\n",
    "4. Infer strong labels on weakly labeled data (20 points)\n",
    "5. Retrain model and evaluate on test data (20 points)\n",
    "\n",
    "An additional 10 points will be based on how well organized, commented, and readable your code is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9425c12a-9696-4f63-9996-03e072b246c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d910fbfc-a073-42b9-8033-d1c85751e587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa as lb\n",
    "from pathlib import Path\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9765be7-739d-428d-96fc-55c0dfb2141d",
   "metadata": {},
   "source": [
    "### Part 1: Data Collection & Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed578749-e858-439f-842b-76e4a212bcfa",
   "metadata": {},
   "source": [
    "For the data collection, you will record 10 audio clips:\n",
    "- Training data: 5 recordings.  Five of the audio recordings will be for training.  For these recordings, you should say a random sequence of ten yes's or no's.  For example, one recording might be 'yes no yes yes no no yes yes yes no'.  The recordings do not all have to have the same sequence of yes's and no's.  When recording your speaking, please include a variable-length silence in between each word.\n",
    "- Test data: 5 recordings.  Five of the audio recordings will be for testing.  For these recordings, you should say a variable-length sequence of yes's and no's.  For example, one recording might be 'yes no' and another might be 'no yes yes no no yes no yes'.  Use a variety of different lengths in your sentences.  Again, please include silence in between each word.\n",
    "\n",
    "All audio recordings should be done by a single person on a single device in the same environment (e.g. recorded on a cell phone in your dorm room)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae778194-a306-4238-9fd7-6d3bae3aa69e",
   "metadata": {},
   "source": [
    "Once you have recorded the audio, convert them to a format that librosa can read (e.g. wav or mp3) and put them in a folder entitled 'audio/'.  Name the files train1.mp3, train2.mp3, test1.mp3, test2.mp3, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fed767da-aa15-4f43-b3d7-775fa962a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_DIR = Path('audio') \n",
    "ext = '.mp3' # audio format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "049c2678-6dcb-4c8b-b5a5-c1f21b5dc6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifyAudioData(indir, file_ext):\n",
    "    # verifies that all the needed audio files are present\n",
    "\n",
    "    # check train files\n",
    "    for i in range(5):\n",
    "        curfile = Path(AUDIO_DIR, f'train{i+1}').with_suffix(ext)\n",
    "        assert curfile.is_file(), f'Missing training file {curfile}'\n",
    "\n",
    "    # check test files\n",
    "    for i in range(5):\n",
    "        curfile = Path(AUDIO_DIR, f'test{i+1}').with_suffix(ext)\n",
    "        assert curfile.is_file(), f'Missing test file {curfile}'\n",
    "\n",
    "    print('All required audio data files are present!')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0c46e3d-6dfb-4ab8-9fa2-be1d7d52a2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required audio data files are present!\n"
     ]
    }
   ],
   "source": [
    "verifyAudioData(AUDIO_DIR, ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0016d33b-d4b8-48ae-a7fd-bd3cad703856",
   "metadata": {},
   "source": [
    "Next you will create two different kinds of annotations: weak labels and strong labels.  \n",
    "\n",
    "**Weak labels**.  Create two annotation files for the weak labels.  The file train.transcription should look like:\n",
    "\n",
    "train1.mp3|yes no no yes no yes no no yes no\\\n",
    "train2.mp3|no yes yes yes no no yes no yes no\\\n",
    "...\\\n",
    "train5.mp3|yes no yes yes no no no yes no yes\n",
    "\n",
    "The file test.transcription should look like:\n",
    "\n",
    "test1.mp3|yes no\\\n",
    "test2.mp3|no yes yes no yes\\\n",
    "...\\\n",
    "test5.mp3|yes no yes no no yes no yes yes yes\n",
    "\n",
    "**Strong labels**.  Create one annotation file for train1.mp3 (only) containing strong labels.  The file train1.labels should be in the following format:\n",
    "\n",
    "0.00 1.37 sil\\\n",
    "1.37 2.60 yes\\\n",
    "2.60 5.20 sil\\\n",
    "5.20 6.30 no\\\n",
    "6.30 8.10 sil\\\n",
    "...\\\n",
    "12.30 13.25 yes\\\n",
    "13.25 15.90 sil\n",
    "\n",
    "For annotating timestamps, I recommend that you use Audacity.  Note that we are only annotating one file with strong labels because this process is time-consuming!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "813bb75f-fdb0-447b-85bc-3393e0a054c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOT_DIR = Path('annot')\n",
    "train_transcripts = Path(ANNOT_DIR, 'train.transcription')\n",
    "test_transcripts = Path(ANNOT_DIR, 'test.transcription')\n",
    "train1_labels = Path(ANNOT_DIR, 'train1.labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef09f187-6a0c-4506-a9ec-facf5d8635cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifyAnnotations():\n",
    "    # verifies that all of the needed annotation files are present\n",
    "\n",
    "    assert train_transcripts.is_file(), f'Missing transcription file {train_transcripts}'    \n",
    "    assert test_transcripts.is_file(), f'Missing transcription file {test_transcripts}'    \n",
    "    assert train1_labels.is_file(), f'Missing label file {train1_labels}'\n",
    "    print('All required annotation files are present!')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f22c507-7922-469b-b036-cdb0a95150af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required annotation files are present!\n"
     ]
    }
   ],
   "source": [
    "verifyAnnotations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5278ab12-bfc3-4993-a273-b6b59f1685f4",
   "metadata": {},
   "source": [
    "Using the above format for your label file, you should be able to visualize your labels alongside the audio in Audacity by selecting File --> Import --> Labels.  An example is shown below.  This type of visualization will be useful in evaluating how accurate your alignments and predictions are.\n",
    "\n",
    "![Snapshot](labels_snapshot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f24f13-4548-4334-b418-cb79c487f057",
   "metadata": {},
   "source": [
    "**Graded**: Please include a similar visualization of your data below.  Make sure you include your image file in your submission!\n",
    "\n",
    "\\[INCLUDE VISUALIZATION HERE\\]\n",
    "![Snapshot](test1image.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4db47ae-02e7-4c6f-9dd4-f184ae4f0712",
   "metadata": {},
   "source": [
    "### Part 2: Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b38409a-c5cd-4b2e-8208-d7d2d0f9e686",
   "metadata": {},
   "source": [
    "In this part, you will train an HMM model based (only) on the label file for train1.mp3.  You must complete the implementations of the functions below.  No unit tests will be provided, though, so make sure to check your own work and verify that they are what you expect!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce6c0f1-7346-473c-ba02-c2ca7d8e0008",
   "metadata": {},
   "source": [
    "The function below extracts a feature called mel frequency cepstral coefficients.  Unlike chroma features, which focus on pitch (i.e. fine spectral structure), MFCCs focus on timbre (i.e. rough spectral structure) and are helpful for recognizing speech or distinguishing between different instruments.  You may use the librosa function [librosa.feature.mfcc](https://librosa.org/doc/latest/generated/librosa.feature.mfcc.html) with default arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "094891c1-7dfa-4f7e-9ef9-270fa9762f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeFeatures(audiofile):\n",
    "    \"\"\"\n",
    "    This function extracts mel frequency cepstral coefficients (MFCCs) from a given audio file.\n",
    "        \n",
    "    Inputs\n",
    "        - audiofile: filepath to the audio recording that you want to extract MFCC features from\n",
    "        \n",
    "    Outputs\n",
    "        - O: an F x N array containing MFCC features, where F corresponds to different features and \n",
    "             N corresponds to different audio frames\n",
    "        - hop: the hop size (in seconds) between adjacent frames\n",
    "    \"\"\"\n",
    "\n",
    "    ### INSERT CODE BELOW ###\n",
    "    winsize=1024                    # window size   [samples]\n",
    "    hop_samples=int(winsize//4)     # hop size      [samples]\n",
    "    y, sr = lb.load(audiofile)      # unpack data and sample rate from audiofile\n",
    "    hop_seconds = hop_samples / sr  # hop size      [seconds]\n",
    "    O = lb.feature.mfcc(y=y, win_length = winsize, hop_length = hop_samples ,sr=sr) # compute mfcc features\n",
    "    return (O, hop_seconds)         # return mfcc features and hop size [seconds]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71433ef-357f-4228-bdb9-6fb415630c4c",
   "metadata": {},
   "source": [
    "The function below constructs a mapping between the states and their numeric identifiers.  For ease of grading, please use the following mapping:\n",
    "- sil -> 0\n",
    "- Y -> 1\n",
    "- EH -> 2\n",
    "- S -> 3\n",
    "- N -> 4\n",
    "- OH -> 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5465e689-ae52-47dc-9421-15d7bb9e0ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStateMapping():\n",
    "    \"\"\"\n",
    "    Returns a mapping between the string and numeric representations of the six different states.\n",
    "    \n",
    "    Outputs\n",
    "      - states: a list that contains the states (in order).  This allows you to map from the numeric identifier\n",
    "        to the string representation (e.g. states[3])\n",
    "      - stateStr2id: a dict that maps from the state string representation to its numeric identifier (e.g. stateStr2id['EH'])\n",
    "    \"\"\"\n",
    "    ### INSERT CODE BELOW ###\n",
    "    #raise NotImplementedError\n",
    "    states=[\"sil\",\"Y\",\"EH\",\"S\",\"N\",\"OH\"]\n",
    "    stateStr2id ={\n",
    "        \"sil\":0,\n",
    "        \"Y\":1,\n",
    "        \"EH\":2,\n",
    "        \"S\":3,\n",
    "        \"N\":4,\n",
    "        \"OH\":5     \n",
    "    }\n",
    "    \n",
    "    return (states, stateStr2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30d7fb0-4d62-4305-ac97-eac632a063b1",
   "metadata": {},
   "source": [
    "The function below converts a .labels file into a sequence of states per frame.  You may assume that states in a word have equal duration (e.g. if the word 'yes' lasts 1.2 seconds, you can assume that 'Y', 'EH', and 'S' each have duration 0.4 sec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d241dae0-d815-4059-8314-26b31e6f2305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStatesFromLabelFile(labelfile, hopsize, str2id):\n",
    "    \"\"\"\n",
    "    Reads in a label file and returns a sequence of states for each audio frame.  For any given word, it assumes\n",
    "    that the constituent states all have equal duration.  For example, if the word 'yes' lasts 1.2 seconds, the constituent\n",
    "    states 'Y', 'EH', and 'S' are assumed to each have duration 0.4 seconds.\n",
    "    \n",
    "    Inputs\n",
    "      - labelfile: filepath to the .labels or .forcealign file \n",
    "      - hopsize: the hop size in seconds between adjacent audio frames\n",
    "      - str2id: dict that maps from the state's string representation to its numeric representation\n",
    "\n",
    "    Outputs\n",
    "      - S: list containing the sequence of numeric states for each audio frame\n",
    "    \"\"\"\n",
    "    ### INSERT CODE BELOW ###\n",
    "    f = open(labelfile)\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    lastLine = lines[-1].split('\\t') # extract the end time of the last label to determine the length of S\n",
    "    endtime = float(lastLine[1])\n",
    "\n",
    "    S = np.zeros(int(endtime//hopsize))\n",
    "\n",
    "    states = [\"sil\", \"Y\", \"EH\", \"S\", \"N\", \"OH\"]\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        line = lines[i].split('\\t')   # split the label into [start time, end time, label identifier]\n",
    "        t1 = float(line[0])            # extract start time (convert from string to float)\n",
    "        t2 = float(line[1])            # extract end time\n",
    "        dur = t2 - t1                   # compute duration of label \n",
    "        ind = np.zeros(4)\n",
    "        label = line[2]\n",
    "        \n",
    "\n",
    "        if 'sil' in label:           # check if 'sil' is in the label rather than checking equality to deal with inconsistent newline characters\n",
    "            for i in range(2):  # generate beginning and end indices bc only one silent state\n",
    "                ind[i] = int((t1 + i*dur) // hopsize)\n",
    "            S[int(ind[0]): int(ind[1])] = str2id[states[0]]\n",
    "\n",
    "        elif 'yes' in label:\n",
    "            for i in range(4):  # generate equally spaced indices to separate 3 'yes' states\n",
    "                ind[i] = int((t1 + i/3*dur) // hopsize)\n",
    "            for i in range(3):  # assign states to S matrix\n",
    "                S[int(ind[i]): int(ind[i+1])] = str2id[states[1+i]]\n",
    "\n",
    "        elif 'no' in label:\n",
    "            for i in range(3):  # generate equally spaced indices to separate 2 'no' states\n",
    "                ind[i] = int((t1 + i/2*dur) // hopsize)\n",
    "            for i in range(2):  # assign states to S matrix\n",
    "                S[int(ind[i]): int(ind[i+1])] = str2id[states[i+4]] \n",
    "    \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "288f5d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file= \"annot/train1.labels\"\n",
    "hop = 0.1\n",
    "states, str2id = getStateMapping()\n",
    "S = getStatesFromLabelFile(labelfile = file, hopsize = hop, str2id = str2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1890cf-f95d-49de-8e15-820fa48f697d",
   "metadata": {},
   "source": [
    "This is a good place to verify that your functions are producing correct outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a36f2442",
   "metadata": {},
   "outputs": [],
   "source": [
    "O, hop = computeFeatures(Path(AUDIO_DIR, 'train1.mp3'))\n",
    "states, stateStr2id = getStateMapping()\n",
    "S = getStatesFromLabelFile(train1_labels, hop, stateStr2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa87234-985b-4d77-82cc-7d2f53b31dac",
   "metadata": {},
   "source": [
    "The function below trains an HMM given a list of observations (i.e. MFCC feature matrices) and corresponding states.  A few helpful tips:\n",
    "- In this part of the assignment, you will only train the model on the train1.mp3 example, but your function below should be able to handle multiple training examples so that you can reuse this function in part 5.\n",
    "- You may assume that recordings always begin in the silent state.\n",
    "- You should assume that the emission probability model is a multivariate Gaussian model.\n",
    "- You should decompose this function and define other sub-functions as needed to keep your code neat and organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b7711d0-d3d2-4857-b4ce-a207d041af51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainHMM(O_list, S_list, states):\n",
    "    \"\"\"\n",
    "    Trains an HMM given a list of observations and corresponding states.  The HMM assumes a multivariate Gaussian\n",
    "    emission probability model.\n",
    "\n",
    "    Inputs\n",
    "      - O_list: a list of matrices, where each matrix contains the MFCC coefficients for a single training recording\n",
    "      - S_list: a list of arrays, where each array specifies the states in each audio frame for a single training recording\n",
    "      - states: a list specifying the states in their string representation\n",
    "\n",
    "    Outputs\n",
    "      - A: the state transition probability matrix\n",
    "      - pi: the distribution of the initial state\n",
    "      - means: a matrix where each row specifies the mean of the distribution for a single state\n",
    "      - covars: a 3D tensor where the first index specifies a state, and the remaining two indices specify the covariance \n",
    "        matrix for the state's distribution\n",
    "    \"\"\"\n",
    "    ### INSERT CODE BELOW ###\n",
    "\n",
    "    \"\"\"\n",
    "    Note:\n",
    "    Phases of function:\n",
    "      1. determine A matrix by going through the entire list of states. Index aij will be # of times there is a state transition from i to j / total number of transitions from i to another state\n",
    "      2. Determine initial pi by perhaps averaging probabilities over all states, or simply choosing an initial probability\n",
    "      3. Generate list of means: where mu_i corresponds to the average of all observations for state i\n",
    "      4. From the means, generate covariance matrices for all i states\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #TODO O list is a list of observations for multiple recordings. Need to update function to reflect that.\n",
    "    state_names,stateStr2id=getStateMapping(states)\n",
    "\n",
    "    num_states=len(states)\n",
    "    A_sum = np.zeros((num_states,num_states)) \n",
    "    A = np.zeros((num_states,num_states)) \n",
    "    observations_sum = np.zeros(num_states,np.shape(O_record)[1]) #summed observation for each state\n",
    "    \n",
    "    for i in range(len(S_list)):\n",
    "      S_record=S_list[i]\n",
    "      O_record=O_list[i]\n",
    "      A=A[i]+countTransitions(S_record,states,stateStr2id) #add to the sum of A's\n",
    "      state_count_list=countNumStates(S_record, states) #matrix containing the number of time a state appears\n",
    "    \n",
    "      (O_record,S_record,states,state_count)\n",
    "\n",
    "    #computing the A matrix for the \n",
    "    transitions_sum = np.einsum(\"ij->i\",A) #each element is the total number of transitions from each state index i\n",
    "    for i in range(len(transitions_sum)):\n",
    "      A[i,:]=A[i,:]/transitions_sum[i] #now divide by the total number of transitions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #raise NotImplementedError\n",
    "    return (A, pi, means, covars)\n",
    "\n",
    "def findPi(S_list, states):\n",
    "  \"\"\"\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "def countTransitions(S_record,states,mapping):\n",
    "  num_states=len(states)\n",
    "  num_transitions_mat=np.zeros((num_states,num_states)) \n",
    "  for i in range(len(S_record)-1):\n",
    "    current_state=S_record[i] #string of current state\n",
    "    next_state=S_record[i+1] #string of next state\n",
    "    current_state_idx=mapping[current_state] #index of current state for matrix\n",
    "    next_state_idx=mapping[next_state] #index of next state for matrix\n",
    "    num_transitions_mat[current_state_idx,next_state_idx]=num_transitions_mat[current_state_idx,next_state_idx]+1 \n",
    "  return num_transitions_mat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def findA(S_list,states):\n",
    "\n",
    "\n",
    "  #TODO this assumes that state_names and states are the same. if not working, see if the states are arranged in the same order\n",
    "  state_names,stateStr2id=getStateMapping(states)\n",
    "  num_states=len(states)\n",
    "  num_transitions_mat=np.zeros((num_states,num_states)) #keep count of the state transitions\n",
    "  A_mat=np.zeros((num_states,num_states)) #keep count of the state transitions\n",
    "  #matrix formatted as row=state from, col= state to\n",
    "\n",
    "  for i in range(len(S_list)-1):\n",
    "    current_state=S_list[i] #string of current state\n",
    "    next_state=S_list[i+1] #string of next state\n",
    "    current_state_idx=stateStr2id[current_state] #index of current state for matrix\n",
    "    next_state_idx=stateStr2id[next_state] #index of next state for matrix\n",
    "\n",
    "    #increment count of transition in the matrix\n",
    "    num_transitions_mat[current_state_idx,next_state_idx]=num_transitions_mat[current_state_idx,next_state_idx]+1 \n",
    "    \n",
    "  transitions_sum = np.einsum(\"ij->i\",num_transitions_mat) #each element is the total number of transitions from each state index i\n",
    "  #TODO: if not working, switch to ij->j\n",
    "\n",
    "  #now divide each row (ai0, ai1, ai2, i...) by sum of aij\n",
    "  for i in range(len(num_transitions_mat)):\n",
    "    A_mat[i,:]=num_transitions_mat[i,:]/transitions_sum[i]\n",
    "  return A_mat\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def countNumStates(S_list,states):\n",
    "  state_names,stateStr2id=getStateMapping(states)\n",
    "  state_count =np.zeros(len(states)) #keeps count of how many times a state appears\n",
    "  for i in range(len(S_list)):\n",
    "    current_state=S_list[i] \n",
    "    current_state_idx=stateStr2id[current_state]\n",
    "    state_count[current_state_idx]=state_count[current_state_idx]+1\n",
    "  return state_count\n",
    "\n",
    "\n",
    "def countObservations(O_record,S_record,states,mapping):\n",
    "  sum_observations = np.zeros(len(states),np.shape(O_record)[1]) #summed observation for each state\n",
    "  for i in range(len(O_record)):\n",
    "    current_state=S_record[i]#current state\n",
    "    current_observation=O_record[i]\n",
    "    current_state_idx=mapping[current_state]\n",
    "    sum_observations[current_state_idx,:]=sum_observations[current_state_idx,:]+current_observation #add current observation to running count\n",
    "  return sum_observations\n",
    "\n",
    "\n",
    "def generateMeans(O_record,S_record,states,state_count):\n",
    "  \"\"\"\n",
    "  todo: determine if there is a way to vectorize this\n",
    "  \"\"\"\n",
    "  #TODO this assumes that state_names and states are the same. if not working, see if the states are arranged in the same order\n",
    "  state_names,stateStr2id=getStateMapping(states)\n",
    "  sum_observations = np.zeros(len(states),np.shape(O_record)[1]) #summed observation for each state\n",
    "  average_obs = np.zeros(len(states),np.shape(O_record)[1]) #summed observation for each state\n",
    "  #state_count =countNumStates(S_list,states) #keeps count of how many times a state appears\n",
    "\n",
    "\n",
    "  for i in range(len(O_record)):\n",
    "    current_state=S_record[i] \n",
    "    current_observation=O_record[i]\n",
    "    sum_observations[current_state,:]=sum_observations[current_state,:]+current_observation #add current observation to running count\n",
    "\n",
    "  for i in range(len(states)):\n",
    "    average_obs[i,:]=sum_observations[i,:]/state_count[i] #average the observations\n",
    "  \n",
    "  return average_obs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generateCovariances(O_list,states,means,state_count):\n",
    "  state_names,stateStr2id=getStateMapping(states)\n",
    "  obs_size=np.shape(O_list)[1]\n",
    "  #state_count =countNumStates(S_list,states) #keeps count of how many times a state appears\n",
    "  variances = np.zeros((len(states),obs_size,obs_size))#variances for each state\n",
    "\n",
    "  \n",
    "\n",
    "  for i in range(len(states)):\n",
    "    dif_vector = O_list-means[i] #subtract the average observation of state i from all of the observations over time\n",
    "    variances[i,:,:]=(1/(state_count[i]-1))*np.sum(dif_vector@dif_vector.T)\n",
    "\n",
    "  return variances\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e758ac-d4ac-43da-bb90-a53c413f68dd",
   "metadata": {},
   "source": [
    "Use the functions defined above to train an HMM model on the train1.labels file (only).  For grading purposes, please print out the following variables (and make sure your submitted notebook is showing the values): \n",
    "- the state transition probability matrix A\n",
    "- the means for all six states\n",
    "- the covariance matrix for 'sil'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70615059-14a1-458b-9c2a-bfb7a88d9e2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'audio\\\\train1.labels.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m train1_obs, hop \u001b[38;5;241m=\u001b[39m computeFeatures(audiofile \u001b[38;5;241m=\u001b[39m audiofile) \u001b[38;5;66;03m# compute observations and hop size from audio file\u001b[39;00m\n\u001b[0;32m      8\u001b[0m states, stateStr2id \u001b[38;5;241m=\u001b[39m getStateMapping() \u001b[38;5;66;03m# retrieve list of possible states and mapping between string and integer state representation\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m train1_states \u001b[38;5;241m=\u001b[39m getStatesFromLabelFile(labelfile \u001b[38;5;241m=\u001b[39m labelfile, hopsize \u001b[38;5;241m=\u001b[39m hop, str2id \u001b[38;5;241m=\u001b[39m stateStr2id)\n",
      "Cell \u001b[1;32mIn[11], line 16\u001b[0m, in \u001b[0;36mgetStatesFromLabelFile\u001b[1;34m(labelfile, hopsize, str2id)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mReads in a label file and returns a sequence of states for each audio frame.  For any given word, it assumes\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mthat the constituent states all have equal duration.  For example, if the word 'yes' lasts 1.2 seconds, the constituent\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m  - S: list containing the sequence of numeric states for each audio frame\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m### INSERT CODE BELOW ###\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(labelfile)\n\u001b[0;32m     17\u001b[0m lines \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[0;32m     19\u001b[0m lastLine \u001b[38;5;241m=\u001b[39m lines[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# extract the end time of the last label to determine the length of S\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rburg\\anaconda3\\envs\\E207_Spr24\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'audio\\\\train1.labels.txt'"
     ]
    }
   ],
   "source": [
    "### INSERT AS MANY CELLS AS NEEDED BELOW ###\n",
    "## RAFAEL \n",
    "audiofile = Path(AUDIO_DIR, \"train1.mp3\")\n",
    "labelfile = Path(AUDIO_DIR, \"train1.labels.txt\")\n",
    "\n",
    "train1_obs, hop = computeFeatures(audiofile = audiofile) # compute observations and hop size from audio file\n",
    "\n",
    "states, stateStr2id = getStateMapping() # retrieve list of possible states and mapping between string and integer state representation\n",
    "\n",
    "train1_states = getStatesFromLabelFile(labelfile = labelfile, hopsize = hop, str2id = stateStr2id) # generate state list for audio file from strong labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af74e81-44c4-45a1-9a71-c394c15fc49d",
   "metadata": {},
   "source": [
    "**Graded**: Print out A below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83910bd-20ea-40c3-bfeb-201ec015cb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f250db-3945-4c92-8a8d-ea8145fd4eca",
   "metadata": {},
   "source": [
    "**Graded**: Print out the state distribution means below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e3b0e9-1db7-4917-8b12-08c21185ef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f209f330-b8fb-4693-9edb-148e8c396447",
   "metadata": {},
   "source": [
    "**Graded**: Print out the covariance matrix for 'sil' below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bccf546-7fb0-44a3-bf50-42cf09e0e893",
   "metadata": {},
   "outputs": [],
   "source": [
    "covars[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fcf5f5-274f-4d2e-a23e-e82cad3e8ec4",
   "metadata": {},
   "source": [
    "### Part 3: Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798cdd39-dee7-4191-a52a-b4f963af760d",
   "metadata": {},
   "source": [
    "In this part, we will use our trained model from part 2 to estimate the state sequence on test recordings.  You must complete the implementations of the functions below.  Again, no unit tests will be provided, so make sure to check your own work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab743ab-4abb-454a-9a2d-f6a2e7d141c1",
   "metadata": {},
   "source": [
    "The function below calculates a pairwise similarity matrix, assuming a Gaussian emission probability model.  You may use the scipy implementation of [multivariate_normal](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html) in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9daae01b-2c82-4a09-920d-248160ad58bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAFAEL \n",
    "def calcSimilarity_multivariateGaussian(O, means, covars):\n",
    "    \"\"\"\n",
    "    Calculates the matrix of likelihoods for a sequence of observations and a set of multivariate Gaussian models.\n",
    "\n",
    "    Inputs\n",
    "      - O: an D x N observation matrix, where D is the dimensionality of the feature representation and N is the number\n",
    "        of observations\n",
    "      - means: an M x D matrix specifying the distribution means, where M is the number of multivariate Gaussian models \n",
    "      - covars: an M x D x D array specifying the distribution covariance matrices, where the first index specifies the model\n",
    "        and the remaining two indices specify the model's covariance matrix\n",
    "\n",
    "    Outputs\n",
    "      - prob: a M x N matrix specifying model likelihoods, where M corresponds to the different models and where N corresponds\n",
    "        to the different observations\n",
    "    \"\"\"\n",
    "    ### INSERT CODE BELOW ###\n",
    "    \n",
    "    M = means.shape[0]  # unpack required matrix dimensions\n",
    "    N = O.shape[1]      \n",
    "\n",
    "    prob = np.zeros((M,N))\n",
    "\n",
    "    for row in range(M):  # loop through models\n",
    "        model_E = means[row, :]   # unpack current model mean\n",
    "        model_Sig = covars[row, :, :]   # unpack current model covariance\n",
    "        dist = multivariate_normal(mean = model_E, cov = model_Sig)   # generate gaussian for current model\n",
    "        prob[row, :] = dist.pdf(O)  # evaluate current model distribution at the observations \n",
    "\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0c1aa5-0c92-434f-936d-475a0028e161",
   "metadata": {},
   "source": [
    "The function below implements the Viterbi algorithm from scratch.  Since there are lots of implementations of Viterbi online, you should not consult any direct implementations.  If you are unable to complete this part on your own, you may consult an online implementation for a grade deduction.  If you do so, please cite the resource and describe the extent of the assistance so that points may be deducted appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb6ec49-704a-46e4-be5a-cc3b3c95fb3f",
   "metadata": {},
   "source": [
    "**Graded**: Please cite any resources you consulted in implementing the function below, and the extent of the assistance: \n",
    "\n",
    "\\<PUT RESPONSE HERE\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f87d041e-f0df-419f-92c0-78ce62f9b695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.30258509        -inf        -inf        -inf        -inf]\n",
      " [       -inf        -inf        -inf        -inf        -inf]\n",
      " [       -inf        -inf        -inf        -inf        -inf]\n",
      " [       -inf        -inf        -inf        -inf        -inf]]\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rburg\\AppData\\Local\\Temp\\ipykernel_12720\\3032479501.py:23: RuntimeWarning: divide by zero encountered in log\n",
      "  D[:,0] = np.log(pi) + np.log(prob[:,0]) # Initialize first column of D\n",
      "C:\\Users\\rburg\\AppData\\Local\\Temp\\ipykernel_12720\\3032479501.py:29: RuntimeWarning: divide by zero encountered in log\n",
      "  scores = D[:,col-1] + np.log(trans) + np.log(probs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAFAEL\n",
    "def viterbi(prob, A, pi):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "      - prob: a M x N matrix specifying model likelihoods, where M is the number of models and N is the number of observations\n",
    "      - A: an M x M transition probability matrix\n",
    "      - pi: a length M array specifying the initial state probability distribution\n",
    "        \n",
    "    Outputs\n",
    "      - S_est: the estimated sequence of (numeric) states\n",
    "    \"\"\"\n",
    "    ### INSERT CODE BELOW ###\n",
    "\n",
    "    M,N = prob.shape\n",
    "\n",
    "    pi = pi.reshape(M,) # make sure pi is a column vector\n",
    "    \n",
    "    #### CONSTRUCT D AND B MATRICES\n",
    "\n",
    "    D = np.zeros((M, N))  # Allocate cumulative path score matrix D\n",
    "    B = np.zeros((M, N))  # Allocate backrace matrix B\n",
    "\n",
    "    D[:,0] = np.log(pi) + np.log(prob[:,0]) # Initialize first column of D\n",
    "    \n",
    "    for col in range(1,N):\n",
    "        for row in range(M): # iterate through remaining entries\n",
    "            trans = A[:,row] # column vector of transitions from state 1,2,...,M -> state specified by current row\n",
    "            probs = prob[:,col] # column vector of observation probabilities\n",
    "            scores = D[:,col-1] + np.log(trans) + np.log(probs)\n",
    "            D[row, col] = np.max(scores)    # assign max of possible scores to cumulative score matrix entry\n",
    "            B[row, col] = np.argmax(scores) # assign backpointer to be index of max possible score\n",
    "\n",
    "    print(D)\n",
    "    print(B)\n",
    "\n",
    "    #### EXTRACT ESTIMATED PATH THROUGH D\n",
    "    S_est = np.zeros(N)\n",
    "    S_est[0] = np.argmax(D[:,-1])\n",
    "\n",
    "    for i in range(1,N):\n",
    "        S_est[i] = B[int(S_est[i-1]),N-i]\n",
    "\n",
    "    return S_est\n",
    "\n",
    "M = 4\n",
    "N = 5\n",
    "prob = np.zeros((M,N))\n",
    "prob[0,0] = 1\n",
    "prob[1,1] = 1\n",
    "prob[0,2] = 1\n",
    "prob[2,3] = 1\n",
    "prob[3,4] = 1\n",
    "pi = np.array([0.1, 0.1, 0.2, 0.3])\n",
    "A = np.zeros((M, M)) + 0.25\n",
    "\n",
    "viterbi(prob, A, pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03b80ff-de38-4f54-904d-9f6752f2dddf",
   "metadata": {},
   "source": [
    "Using the two functions above, estimate the state sequence for each test recording and generate the corresponding .labels file (it can have a different extension but should have the same format so as to be readable by Audacity).  Include a visualization of your estimated states alongside the audio in Audacity (as shown above).  You may use as many code cells as needed, and be sure to decompose your code appropriately!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0523a6-49d1-419b-a78b-148305c5bddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT AS MANY CELLS AS NEEDED BELOW ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeba3454-c5d7-4d4f-b9dc-2919cf0bfed4",
   "metadata": {},
   "source": [
    "Comment on what you observe in your estimated state sequence, and propose some ideas on how you might improve the system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0d3d05-9e9b-4af6-b66f-dda20a0bfb9b",
   "metadata": {},
   "source": [
    "**Graded**: \n",
    "\n",
    "\\<INSERT VISUALIZATION & RESPONSE HERE\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e0a077-b593-4633-b377-45aad81fe9c7",
   "metadata": {},
   "source": [
    "### Part 4: Forced Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b966fb14-3e66-4808-806f-5ee7f6a64f87",
   "metadata": {},
   "source": [
    "In this part, you will perform forced alignment to determine the correspondence between the states in a given word-level transcription and the corresponding audio recording.  Your goal is to implement the function below, and then use it to determine the state-level alignment for train1.mp3.  Make sure to decompose your function appropriately!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3db6a4-c4b1-4925-b5a7-8dabd342dce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forcedAlignment(audiofile, word_transcript, model, stateStr2id):\n",
    "    \"\"\"\n",
    "    Performs forced alignment between a given word-level transcription and the corresponding audio recording.\n",
    "\n",
    "    Inputs\n",
    "      - audiofile: filepath to the audio recording\n",
    "      - word_transcript: a string indicating the word-level transcription.  The transcription should only\n",
    "        contain 'yes' and 'no'; the function will raise an error if it contains anything other than these two words\n",
    "      - model: tuple of (A, pi, means, covars) specifying the trained HMM\n",
    "      - stateStr2id: a dict that maps from the state string representation to its numeric identifier (e.g. stateStr2id['EH'])\n",
    "\n",
    "    Output\n",
    "      - alignment: an array specifying the coordinates of the forced alignment\n",
    "    \"\"\"\n",
    "    ### INSERT CODE BELOW ###\n",
    "    raise NotImplementedError\n",
    "\n",
    "    return alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3ab7c3-3365-4811-84a7-6b21654e5f56",
   "metadata": {},
   "source": [
    "Once you have implemented the forced alignment function above, use it to estimate the state-level alignment for train1.mp3.  Visualize the predicted alignment alongside the audio in Audacity, and also include the word-level alignment from part 1 (that you manually created).  Comment on how the forced alignmend method improves the quality of the alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342c62a1-f790-4209-bc1b-3bc75b0cda24",
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT AS MANY CELLS AS NEEDED BELOW ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33abd41-80f9-441e-a1c8-2f98de6397c2",
   "metadata": {},
   "source": [
    "**Graded**: Include the visualization below and comment on what you observe.\n",
    "\n",
    "\\[Show predicted alignment in Audacity\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458f0c77-ca26-408d-ab0d-38eace106eb6",
   "metadata": {},
   "source": [
    "### Part 5: Retrain Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8868303-cf1a-44d1-91d9-da0cf2ee2baf",
   "metadata": {},
   "source": [
    "In the last part of the assignment, you will use your initial model from part 2, perform forced alignment to generate .forcealign files for all weakly labeled training data, re-train your HMM, and then perform inference on the test data with the new model.  Provide a snapshot in Audacity comparing the predictions from part 3 and part 5 on a single test file.  Comment on any differences you observe, what the re-trained model is doing well, and where the re-trained model is making errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626c7be8-0316-4b3a-8e2e-f61c738ae025",
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT AS MANY CELLS AS NEEDED BELOW ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d617af9-85e1-451e-ba5a-75e06e869886",
   "metadata": {},
   "source": [
    "**Graded**:  \n",
    "\\[INSERT VISUALIZATION & COMMENTS HERE\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a68883-689e-42c5-8f7b-279b1bcae712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "E207_Spr24",
   "language": "python",
   "name": "e207_spr24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
