{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c91d93-14ad-4ce8-b3da-454c16e66889",
   "metadata": {},
   "source": [
    "# Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63b1938-daaf-483d-b86c-9bb60361ce91",
   "metadata": {},
   "source": [
    "In this homework assignment you will train and evaluate a toy speech recognizer that can recognize 'yes' and 'no'.  As we move into the latter half of the course, note that the assignments will have less and less hand holding.  The goal is for you to become more and more independent, so that you are ready to work independently on the final project.  This assignment should be done in pairs and only one submission needs to be submitted on Gradescope for each team.  In your submission, you should create a single .zip file that contains your jupyter notebook, audio data, annotation data, and generated prediction files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b40e47-d050-46db-9870-84796af00cca",
   "metadata": {},
   "source": [
    "Please indicate the team member names here: ________________________\n",
    "\n",
    "How many hours you each spent on this assignment: _________ (partner 1), __________ (partner 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc250c8-be3d-4ee3-903b-041c64d7c14c",
   "metadata": {},
   "source": [
    "This assignment will be broken down into 5 main sections:\n",
    "1. Collect data & annotate (10 points)\n",
    "2. Train model using manual annotations (20 points)\n",
    "3. Perform inference on test data (20 points)\n",
    "4. Infer strong labels on weakly labeled data (20 points)\n",
    "5. Retrain model and evaluate on test data (20 points)\n",
    "\n",
    "An additional 10 points will be based on how well organized, commented, and readable your code is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9425c12a-9696-4f63-9996-03e072b246c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d910fbfc-a073-42b9-8033-d1c85751e587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.seterr(divide='ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa as lb\n",
    "from pathlib import Path\n",
    "from scipy.stats import multivariate_normal\n",
    "import glob\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9765be7-739d-428d-96fc-55c0dfb2141d",
   "metadata": {},
   "source": [
    "### Part 1: Data Collection & Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed578749-e858-439f-842b-76e4a212bcfa",
   "metadata": {},
   "source": [
    "For the data collection, you will record 10 audio clips:\n",
    "- Training data: 5 recordings.  Five of the audio recordings will be for training.  For these recordings, you should say a random sequence of ten yes's or no's.  For example, one recording might be 'yes no yes yes no no yes yes yes no'.  The recordings do not all have to have the same sequence of yes's and no's.  When recording your speaking, please include a variable-length silence in between each word.\n",
    "- Test data: 5 recordings.  Five of the audio recordings will be for testing.  For these recordings, you should say a variable-length sequence of yes's and no's.  For example, one recording might be 'yes no' and another might be 'no yes yes no no yes no yes'.  Use a variety of different lengths in your sentences.  Again, please include silence in between each word.\n",
    "\n",
    "All audio recordings should be done by a single person on a single device in the same environment (e.g. recorded on a cell phone in your dorm room)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae778194-a306-4238-9fd7-6d3bae3aa69e",
   "metadata": {},
   "source": [
    "Once you have recorded the audio, convert them to a format that librosa can read (e.g. wav or mp3) and put them in a folder entitled 'audio/'.  Name the files train1.mp3, train2.mp3, test1.mp3, test2.mp3, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fed767da-aa15-4f43-b3d7-775fa962a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_DIR = Path('audio') \n",
    "ext = '.mp3' # audio format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "049c2678-6dcb-4c8b-b5a5-c1f21b5dc6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifyAudioData(indir, file_ext):\n",
    "    # verifies that all the needed audio files are present\n",
    "\n",
    "    # check train files\n",
    "    for i in range(5):\n",
    "        curfile = Path(AUDIO_DIR, f'train{i+1}').with_suffix(ext)\n",
    "        assert curfile.is_file(), f'Missing training file {curfile}'\n",
    "\n",
    "    # check test files\n",
    "    for i in range(5):\n",
    "        curfile = Path(AUDIO_DIR, f'test{i+1}').with_suffix(ext)\n",
    "        assert curfile.is_file(), f'Missing test file {curfile}'\n",
    "\n",
    "    print('All required audio data files are present!')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0c46e3d-6dfb-4ab8-9fa2-be1d7d52a2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required audio data files are present!\n"
     ]
    }
   ],
   "source": [
    "verifyAudioData(AUDIO_DIR, ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0016d33b-d4b8-48ae-a7fd-bd3cad703856",
   "metadata": {},
   "source": [
    "Next you will create two different kinds of annotations: weak labels and strong labels.  \n",
    "\n",
    "**Weak labels**.  Create two annotation files for the weak labels.  The file train.transcription should look like:\n",
    "\n",
    "train1.mp3|yes no no yes no yes no no yes no\\\n",
    "train2.mp3|no yes yes yes no no yes no yes no\\\n",
    "...\\\n",
    "train5.mp3|yes no yes yes no no no yes no yes\n",
    "\n",
    "The file test.transcription should look like:\n",
    "\n",
    "test1.mp3|yes no\\\n",
    "test2.mp3|no yes yes no yes\\\n",
    "...\\\n",
    "test5.mp3|yes no yes no no yes no yes yes yes\n",
    "\n",
    "**Strong labels**.  Create one annotation file for train1.mp3 (only) containing strong labels.  The file train1.labels should be in the following format:\n",
    "\n",
    "0.00 1.37 sil\\\n",
    "1.37 2.60 yes\\\n",
    "2.60 5.20 sil\\\n",
    "5.20 6.30 no\\\n",
    "6.30 8.10 sil\\\n",
    "...\\\n",
    "12.30 13.25 yes\\\n",
    "13.25 15.90 sil\n",
    "\n",
    "For annotating timestamps, I recommend that you use Audacity.  Note that we are only annotating one file with strong labels because this process is time-consuming!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "813bb75f-fdb0-447b-85bc-3393e0a054c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOT_DIR = Path('annot')\n",
    "train_transcripts = Path(ANNOT_DIR, 'train.transcription')\n",
    "test_transcripts = Path(ANNOT_DIR, 'test.transcription')\n",
    "train1_labels = Path(ANNOT_DIR, 'train1.labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef09f187-6a0c-4506-a9ec-facf5d8635cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifyAnnotations():\n",
    "    # verifies that all of the needed annotation files are present\n",
    "\n",
    "    assert train_transcripts.is_file(), f'Missing transcription file {train_transcripts}'    \n",
    "    assert test_transcripts.is_file(), f'Missing transcription file {test_transcripts}'    \n",
    "    assert train1_labels.is_file(), f'Missing label file {train1_labels}'\n",
    "    print('All required annotation files are present!')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f22c507-7922-469b-b036-cdb0a95150af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required annotation files are present!\n"
     ]
    }
   ],
   "source": [
    "verifyAnnotations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5278ab12-bfc3-4993-a273-b6b59f1685f4",
   "metadata": {},
   "source": [
    "Using the above format for your label file, you should be able to visualize your labels alongside the audio in Audacity by selecting File --> Import --> Labels.  An example is shown below.  This type of visualization will be useful in evaluating how accurate your alignments and predictions are.\n",
    "\n",
    "![Snapshot](labels_snapshot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f24f13-4548-4334-b418-cb79c487f057",
   "metadata": {},
   "source": [
    "**Graded**: Please include a similar visualization of your data below.  Make sure you include your image file in your submission!\n",
    "\n",
    "\\[INCLUDE VISUALIZATION HERE\\]\n",
    "![Snapshot](test1image.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4db47ae-02e7-4c6f-9dd4-f184ae4f0712",
   "metadata": {},
   "source": [
    "### Part 2: Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b38409a-c5cd-4b2e-8208-d7d2d0f9e686",
   "metadata": {},
   "source": [
    "In this part, you will train an HMM model based (only) on the label file for train1.mp3.  You must complete the implementations of the functions below.  No unit tests will be provided, though, so make sure to check your own work and verify that they are what you expect!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce6c0f1-7346-473c-ba02-c2ca7d8e0008",
   "metadata": {},
   "source": [
    "The function below extracts a feature called mel frequency cepstral coefficients.  Unlike chroma features, which focus on pitch (i.e. fine spectral structure), MFCCs focus on timbre (i.e. rough spectral structure) and are helpful for recognizing speech or distinguishing between different instruments.  You may use the librosa function [librosa.feature.mfcc](https://librosa.org/doc/latest/generated/librosa.feature.mfcc.html) with default arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "094891c1-7dfa-4f7e-9ef9-270fa9762f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeFeatures(audiofile):\n",
    "    \"\"\"\n",
    "    This function extracts mel frequency cepstral coefficients (MFCCs) from a given audio file.\n",
    "        \n",
    "    Inputs\n",
    "        - audiofile: filepath to the audio recording that you want to extract MFCC features from\n",
    "        \n",
    "    Outputs\n",
    "        - O: an F x N array containing MFCC features, where F corresponds to different features and \n",
    "             N corresponds to different audio frames\n",
    "        - hop: the hop size (in seconds) between adjacent frames\n",
    "    \"\"\"\n",
    "\n",
    "    ### INSERT CODE BELOW ###\n",
    "    winsize=1024                    # window size   [samples]\n",
    "    hop_samples=int(winsize//4)     # hop size      [samples]\n",
    "    y, sr = lb.load(audiofile)      # unpack data and sample rate from audiofile\n",
    "    hop_seconds = hop_samples / sr  # hop size      [seconds]\n",
    "    O = lb.feature.mfcc(y=y, win_length = winsize, hop_length = hop_samples ,sr=sr) # compute mfcc features\n",
    "    return (O, hop_seconds)         # return mfcc features and hop size [seconds]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71433ef-357f-4228-bdb9-6fb415630c4c",
   "metadata": {},
   "source": [
    "The function below constructs a mapping between the states and their numeric identifiers.  For ease of grading, please use the following mapping:\n",
    "- sil -> 0\n",
    "- Y -> 1\n",
    "- EH -> 2\n",
    "- S -> 3\n",
    "- N -> 4\n",
    "- OH -> 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5465e689-ae52-47dc-9421-15d7bb9e0ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStateMapping():\n",
    "    \"\"\"\n",
    "    Returns a mapping between the string and numeric representations of the six different states.\n",
    "    \n",
    "    Outputs\n",
    "      - states: a list that contains the states (in order).  This allows you to map from the numeric identifier\n",
    "        to the string representation (e.g. states[3])\n",
    "      - stateStr2id: a dict that maps from the state string representation to its numeric identifier (e.g. stateStr2id['EH'])\n",
    "    \"\"\"\n",
    "    ### INSERT CODE BELOW ###\n",
    "    #raise NotImplementedError\n",
    "    states=[\"sil\",\"Y\",\"EH\",\"S\",\"N\",\"OH\"]\n",
    "    stateStr2id ={\n",
    "        \"sil\":0,\n",
    "        \"Y\":1,\n",
    "        \"EH\":2,\n",
    "        \"S\":3,\n",
    "        \"N\":4,\n",
    "        \"OH\":5     \n",
    "    }\n",
    "    \n",
    "    return (states, stateStr2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30d7fb0-4d62-4305-ac97-eac632a063b1",
   "metadata": {},
   "source": [
    "The function below converts a .labels file into a sequence of states per frame.  You may assume that states in a word have equal duration (e.g. if the word 'yes' lasts 1.2 seconds, you can assume that 'Y', 'EH', and 'S' each have duration 0.4 sec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d241dae0-d815-4059-8314-26b31e6f2305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStatesFromLabelFile(labelfile, hopsize, str2id):\n",
    "    \"\"\"\n",
    "    Reads in a label file and returns a sequence of states for each audio frame.  For any given word, it assumes\n",
    "    that the constituent states all have equal duration.  For example, if the word 'yes' lasts 1.2 seconds, the constituent\n",
    "    states 'Y', 'EH', and 'S' are assumed to each have duration 0.4 seconds.\n",
    "    \n",
    "    Inputs\n",
    "      - labelfile: filepath to the .labels or .forcealign file \n",
    "      - hopsize: the hop size in seconds between adjacent audio frames\n",
    "      - str2id: dict that maps from the state's string representation to its numeric representation\n",
    "\n",
    "    Outputs\n",
    "      - S: list containing the sequence of numeric states for each audio frame\n",
    "    \"\"\"\n",
    "    ### INSERT CODE BELOW ###\n",
    "    f = open(labelfile)\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    lastLine = lines[-1].split('\\t') # extract the end time of the last label to determine the length of S\n",
    "    endtime = float(lastLine[1])\n",
    "\n",
    "    S = np.zeros(int(endtime//hopsize))\n",
    "\n",
    "    states = [\"sil\", \"Y\", \"EH\", \"S\", \"N\", \"OH\"]\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        line = lines[i].split('\\t')   # split the label into [start time, end time, label identifier]\n",
    "        t1 = float(line[0])            # extract start time (convert from string to float)\n",
    "        t2 = float(line[1])            # extract end time\n",
    "        dur = t2 - t1                   # compute duration of label \n",
    "        ind = np.zeros(4)\n",
    "        label = line[2]\n",
    "        \n",
    "\n",
    "        if 'sil' in label:           # check if 'sil' is in the label rather than checking equality to deal with inconsistent newline characters\n",
    "            for i in range(2):  # generate beginning and end indices bc only one silent state\n",
    "                ind[i] = int((t1 + i*dur) // hopsize)\n",
    "            S[int(ind[0]): int(ind[1])] = str2id[states[0]]\n",
    "\n",
    "        elif 'yes' in label:\n",
    "            for i in range(4):  # generate equally spaced indices to separate 3 'yes' states\n",
    "                ind[i] = int((t1 + i/3*dur) // hopsize)\n",
    "            for i in range(3):  # assign states to S matrix\n",
    "                S[int(ind[i]): int(ind[i+1])] = str2id[states[1+i]]\n",
    "\n",
    "        elif 'no' in label:\n",
    "            for i in range(3):  # generate equally spaced indices to separate 2 'no' states\n",
    "                ind[i] = int((t1 + i/2*dur) // hopsize)\n",
    "            for i in range(2):  # assign states to S matrix\n",
    "                S[int(ind[i]): int(ind[i+1])] = str2id[states[i+4]] \n",
    "    \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "288f5d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file= \"annot/train1.labels\"\n",
    "hop = 0.1\n",
    "states, str2id = getStateMapping()\n",
    "S = getStatesFromLabelFile(labelfile = file, hopsize = hop, str2id = str2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1890cf-f95d-49de-8e15-820fa48f697d",
   "metadata": {},
   "source": [
    "This is a good place to verify that your functions are producing correct outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a36f2442",
   "metadata": {},
   "outputs": [],
   "source": [
    "O, hop = computeFeatures(Path(AUDIO_DIR, 'train1.mp3'))\n",
    "states, stateStr2id = getStateMapping()\n",
    "S = getStatesFromLabelFile(train1_labels, hop, stateStr2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa87234-985b-4d77-82cc-7d2f53b31dac",
   "metadata": {},
   "source": [
    "The function below trains an HMM given a list of observations (i.e. MFCC feature matrices) and corresponding states.  A few helpful tips:\n",
    "- In this part of the assignment, you will only train the model on the train1.mp3 example, but your function below should be able to handle multiple training examples so that you can reuse this function in part 5.\n",
    "- You may assume that recordings always begin in the silent state.\n",
    "- You should assume that the emission probability model is a multivariate Gaussian model.\n",
    "- You should decompose this function and define other sub-functions as needed to keep your code neat and organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b7711d0-d3d2-4857-b4ce-a207d041af51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef generateCovariances(O_list,states,means,state_count):\\n  state_names,stateStr2id=getStateMapping(states)\\n  obs_size=np.shape(O_list)[1]\\n  #state_count =countNumStates(S_list,states) #keeps count of how many times a state appears\\n  variances = np.zeros((len(states),obs_size,obs_size))#variances for each state\\n\\n  \\n\\n  for i in range(len(states)):\\n    dif_vector = O_list-means[i] #subtract the average observation of state i from all of the observations over time\\n    variances[i,:,:]=(1/(state_count[i]-1))*np.sum(dif_vector@dif_vector.T)\\n\\n  return variances\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trainHMM(O_list, S_list, states):\n",
    "    \"\"\"\n",
    "    Trains an HMM given a list of observations and corresponding states.  The HMM assumes a multivariate Gaussian\n",
    "    emission probability model.\n",
    "\n",
    "    Inputs\n",
    "      - O_list: a list of matrices, where each matrix contains the MFCC coefficients for a single training recording\n",
    "      - S_list: a list of arrays, where each array specifies the states in each audio frame for a single training recording\n",
    "      - states: a list specifying the states in their string representation\n",
    "\n",
    "    Outputs\n",
    "      - A: the state transition probability matrix\n",
    "      - pi: the distribution of the initial state\n",
    "      - means: a matrix where each row specifies the mean of the distribution for a single state\n",
    "      - covars: a 3D tensor where the first index specifies a state, and the remaining two indices specify the covariance \n",
    "        matrix for the state's distribution\n",
    "    \"\"\"\n",
    "    ### INSERT CODE BELOW ###\n",
    "\n",
    "    \"\"\"\n",
    "    Note:\n",
    "    Phases of function:\n",
    "      1. determine A matrix by going through the entire list of states. Index aij will be # of times there is a state transition from i to j / total number of transitions from i to another state\n",
    "      2. Determine initial pi by perhaps averaging probabilities over all states, or simply choosing an initial probability\n",
    "      3. Generate list of means: where mu_i corresponds to the average of all observations for state i\n",
    "      4. From the means, generate covariance matrices for all i states\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #TODO O list is a list of observations for multiple recordings. Need to update function to reflect that.\n",
    "    state_names,mapping=getStateMapping()\n",
    "    num_records=len(O_list) #number of records\n",
    "    observation_size=np.shape(O_list)[1]\n",
    "\n",
    "    num_states=len(states)\n",
    "    A_sum = np.zeros((num_states,num_states)) \n",
    "    A = np.zeros((num_states,num_states)) \n",
    "    observations_sum = np.zeros((num_states,observation_size)) #summed observation for each state\n",
    "    total_state_count_list=np.zeros(num_states) #total number of times a state appears for all recordings\n",
    "    means=np.zeros((num_states,observation_size)) #mean of observations across all recordings\n",
    "    covars=np.zeros((num_states,observation_size,observation_size)) #covariance matrices for all recordings\n",
    "\n",
    "    #looping through all recordings \n",
    "    for i in range(num_records):\n",
    "      S_record=S_list[i]\n",
    "      O_record=O_list[i]\n",
    "      A=A[i]+countTransitions(S_record,states) #Sum up all the precursor A matrices to compute values for all recordings\n",
    "      state_count_list=countNumStates(S_record, states) #list containing the number of time a state appears for this file\n",
    "\n",
    "      total_state_count_list=total_state_count_list+state_count_list# add this to the count of number of times state appears for all files\n",
    "      observations_sum=observations_sum+countObservations(O_record,S_record,states) #sum up all the observations to compute the average later\n",
    "\n",
    "\n",
    "\n",
    "    #computing the A matrix for all recordings\n",
    "    transitions_sum = np.einsum(\"ij->i\",A) #each element is the total number of transitions from each state index i\n",
    "    for i in range(len(transitions_sum)):\n",
    "      A[i,:]=A[i,:]/transitions_sum[i] #now divide by the total number of transitions\n",
    "\n",
    "    #computing average observation and covariances\n",
    "    for i in range(num_states):\n",
    "      means[i,:]=observations_sum[i,:]/total_state_count_list[i] \n",
    "\n",
    "\n",
    "      #TODO see if this can be optimized/vectorized \n",
    "      dif_vec = np.array([O_list[k,:,j]-means[i,:] for k in range(num_records) for j in range(np.shape(O_list[k])[1])]).T #vector across records and time\n",
    "      print(np.shape(dif_vec))\n",
    "      covars[i,:,:]=(1/(total_state_count_list[i]-1))*dif_vec@dif_vec.T\n",
    "\n",
    "    pi=total_state_count_list/np.sum(total_state_count_list)\n",
    "\n",
    "    #pi=0\n",
    "\n",
    "    #raise NotImplementedError\n",
    "    return (A, pi, means, covars)\n",
    "\n",
    "def findPi(S_list, states):\n",
    "  \"\"\"\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "def countTransitions(S_record,states):\n",
    "  num_states=len(states)\n",
    "  num_transitions_mat=np.zeros((num_states,num_states)) \n",
    "  for i in range(len(S_record)-1):\n",
    "    current_state=int(S_record[i]) #string of current state\n",
    "    next_state=int(S_record[i+1] )#string of next state\n",
    "\n",
    "    num_transitions_mat[current_state,next_state]=num_transitions_mat[current_state,next_state]+1 \n",
    "\n",
    "  \n",
    "  return num_transitions_mat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def countNumStates(S_list,states):\n",
    "  state_count =np.zeros(len(states)) #keeps count of how many times a state appears\n",
    "  for i in range(len(S_list)):\n",
    "    current_state=int(S_list[i] )\n",
    "    state_count[current_state]=state_count[current_state]+1\n",
    "  return state_count\n",
    "\n",
    "\n",
    "def countObservations(O_record,S_record,states):\n",
    "  \"\"\"\n",
    "  \"\"\"\n",
    "  #TODO O RECORDSAND S RECORDS ARE NOT THE SAME LENGTH, NEED TO FIX\n",
    "  sum_observations = np.zeros((len(states),np.shape(O_record)[0])) #summed observation for each state\n",
    "  print(\"O_record:countObervsations\",np.shape(O_record))\n",
    "  print(\"S_record:countObervsations\",np.shape(S_record))\n",
    "  for i in range(len(S_record)):\n",
    "    current_state=int(S_record[i])#current state\n",
    "    current_observation=O_record[:,i]\n",
    "    sum_observations[current_state,:]=sum_observations[current_state,:]+current_observation #add current observation to running count\n",
    "  return sum_observations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def generateCovariances(O_list,states,means,state_count):\n",
    "  state_names,stateStr2id=getStateMapping(states)\n",
    "  obs_size=np.shape(O_list)[1]\n",
    "  #state_count =countNumStates(S_list,states) #keeps count of how many times a state appears\n",
    "  variances = np.zeros((len(states),obs_size,obs_size))#variances for each state\n",
    "\n",
    "  \n",
    "\n",
    "  for i in range(len(states)):\n",
    "    dif_vector = O_list-means[i] #subtract the average observation of state i from all of the observations over time\n",
    "    variances[i,:,:]=(1/(state_count[i]-1))*np.sum(dif_vector@dif_vector.T)\n",
    "\n",
    "  return variances\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e758ac-d4ac-43da-bb90-a53c413f68dd",
   "metadata": {},
   "source": [
    "Use the functions defined above to train an HMM model on the train1.labels file (only).  For grading purposes, please print out the following variables (and make sure your submitted notebook is showing the values): \n",
    "- the state transition probability matrix A\n",
    "- the means for all six states\n",
    "- the covariance matrix for 'sil'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "70615059-14a1-458b-9c2a-bfb7a88d9e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O_record:countObervsations (20, 960)\n",
      "S_record:countObervsations (958,)\n",
      "(20, 960)\n",
      "(20, 960)\n",
      "(20, 960)\n",
      "(20, 960)\n",
      "(20, 960)\n",
      "(20, 960)\n"
     ]
    }
   ],
   "source": [
    "### INSERT AS MANY CELLS AS NEEDED BELOW ###\n",
    "## RAFAEL \n",
    "audiofile = Path(AUDIO_DIR, \"train1.mp3\")\n",
    "labelfile = Path(ANNOT_DIR, \"train1.labels\")\n",
    "\n",
    "train1_obs, hop = computeFeatures(audiofile = audiofile) # compute observations and hop size from audio file\n",
    "\n",
    "states, stateStr2id = getStateMapping() # retrieve list of possible states and mapping between string and integer state representation\n",
    "\n",
    "train1_states = getStatesFromLabelFile(labelfile = labelfile, hopsize = hop, str2id = stateStr2id) # generate state list for audio file from strong labels\n",
    "\n",
    "O_list=np.array([train1_obs])\n",
    "S_list=np.array([train1_states])\n",
    "\n",
    "A, pi, means, covars = trainHMM(O_list, S_list, states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af74e81-44c4-45a1-9a71-c394c15fc49d",
   "metadata": {},
   "source": [
    "**Graded**: Print out A below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e83910bd-20ea-40c3-bfeb-201ec015cb9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97650131, 0.01305483, 0.        , 0.        , 0.01044386,\n",
       "        0.        ],\n",
       "       [0.        , 0.95419847, 0.04580153, 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.95419847, 0.04580153, 0.        ,\n",
       "        0.        ],\n",
       "       [0.03846154, 0.00769231, 0.        , 0.95384615, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.95652174,\n",
       "        0.04347826],\n",
       "       [0.04444444, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.95555556]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f250db-3945-4c92-8a8d-ea8145fd4eca",
   "metadata": {},
   "source": [
    "**Graded**: Print out the state distribution means below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18e3b0e9-1db7-4917-8b12-08c21185ef7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.00068771e+02,  9.07281810e+01,  2.27610435e+01,\n",
       "         2.27935201e+01,  1.25259130e+01,  1.63838478e+01,\n",
       "         9.15470868e+00,  1.07835291e+01,  5.56201800e+00,\n",
       "         1.07996634e+01,  4.61305195e+00,  6.39504209e+00,\n",
       "         1.67721756e+00,  2.90329360e+00, -1.65897526e+00,\n",
       "         2.36835092e+00,  6.73739975e-01,  3.56804730e+00,\n",
       "        -1.29386509e+00,  3.87393940e+00],\n",
       "       [-2.35154457e+02,  8.15141006e+01, -1.01412326e+01,\n",
       "         4.33034526e+01,  7.34262726e+00, -2.19423592e+01,\n",
       "         1.40825509e+01,  6.90334800e-01, -2.57736560e+00,\n",
       "         9.72487044e+00, -6.22558614e+00,  1.25156841e+01,\n",
       "        -3.68433262e+00, -1.19449810e+01, -1.80965151e+00,\n",
       "        -1.61406514e+00, -9.54488047e-01,  5.18779684e+00,\n",
       "        -3.60673428e+00, -1.42947927e+00],\n",
       "       [-2.25258321e+02,  1.53658334e+01,  3.10831815e+01,\n",
       "        -2.98850779e+00,  1.31041978e+01,  1.05136127e+01,\n",
       "         1.51622953e+00,  1.80121326e+01,  2.50224530e+00,\n",
       "         1.76327166e+01, -4.48647687e+00,  8.27659905e+00,\n",
       "        -2.84214461e+00, -6.01030812e+00, -1.01880093e+01,\n",
       "         2.50178552e+00,  1.76437286e+00,  4.33656829e+00,\n",
       "        -3.20493143e+00, -1.60693729e+00],\n",
       "       [-2.71688566e+02, -2.05526701e+01,  6.43049366e+01,\n",
       "        -3.21588608e+00,  1.52305826e+01,  3.20826282e+01,\n",
       "        -6.89017233e+00,  2.49158514e+01, -9.11964074e+00,\n",
       "         1.52352873e+01,  1.73702917e+00,  9.83694087e+00,\n",
       "         3.72526923e+00,  6.54099871e-01, -7.29436655e+00,\n",
       "         6.03654300e+00,  1.18576963e+00,  3.51196311e+00,\n",
       "        -3.33600770e+00,  4.77457367e+00],\n",
       "       [-2.67121070e+02,  1.03723497e+02, -1.29544718e+01,\n",
       "         3.22081574e+01,  1.91777180e+01, -9.35291584e+00,\n",
       "         8.97545974e+00,  3.59180282e-01, -3.34171631e+00,\n",
       "         1.38551576e+01, -8.29984870e+00,  1.01052337e+01,\n",
       "         1.06743257e+00, -7.60723730e+00, -9.74901707e+00,\n",
       "        -1.72546724e+00, -3.64430206e-01, -7.64088634e-01,\n",
       "        -9.59976880e+00,  3.90922839e-02],\n",
       "       [-2.65738649e+02,  1.35674413e+02, -2.03746551e+01,\n",
       "         5.47237597e+00, -2.05805169e+01,  1.26695942e+00,\n",
       "         2.53285883e+01, -1.83270674e+00, -7.20050596e+00,\n",
       "         1.90094475e+01,  6.85604792e+00,  5.75607932e+00,\n",
       "        -1.78676961e+00, -9.35901294e+00, -1.39169426e+01,\n",
       "        -4.99483027e+00,  4.65617649e+00, -7.36058579e-01,\n",
       "        -2.19963109e+00,  4.05119356e+00]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f209f330-b8fb-4693-9edb-148e8c396447",
   "metadata": {},
   "source": [
    "**Graded**: Print out the covariance matrix for 'sil' below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bccf546-7fb0-44a3-bf50-42cf09e0e893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.33314808e+05,  1.43690129e+04, -4.00505590e+04,\n",
       "        -8.52058755e+03, -1.78578416e+04, -1.76464436e+04,\n",
       "         8.82512009e+03, -9.19332207e+03, -8.50364261e+03,\n",
       "         6.79188466e+03,  1.95713669e+02,  1.71322738e+03,\n",
       "        -3.08742081e+03, -1.01047186e+04, -7.49487354e+03,\n",
       "        -4.64177596e+03,  3.04256367e+03, -1.34849139e+03,\n",
       "        -5.85243118e+02, -4.76720662e+02],\n",
       "       [ 1.43690129e+04,  8.48018258e+04, -4.66171975e+04,\n",
       "        -2.20743588e+03, -2.53854779e+04, -1.10269392e+04,\n",
       "         1.72626343e+04, -1.33116446e+04, -2.28998885e+03,\n",
       "         3.30515971e+03,  5.58914839e+03, -3.11868380e+03,\n",
       "        -1.68392752e+03, -6.26896329e+03, -5.27132720e+03,\n",
       "        -6.10281803e+03,  2.80838707e+03, -2.97552306e+03,\n",
       "         8.41416268e+02,  1.09302230e+03],\n",
       "       [-4.00505590e+04, -4.66171975e+04,  3.86798475e+04,\n",
       "         8.78283414e+02,  1.55968170e+04,  1.14357326e+04,\n",
       "        -1.05484749e+04,  9.10297246e+03,  1.82186170e+03,\n",
       "        -3.28726867e+03, -3.02037582e+03,  1.00480188e+03,\n",
       "         1.59981882e+03,  5.83110620e+03,  3.56455725e+03,\n",
       "         3.97395575e+03, -2.24361647e+03,  1.21231577e+03,\n",
       "        -5.26348767e+02, -3.18616504e+02],\n",
       "       [-8.52058755e+03, -2.20743588e+03,  8.78283414e+02,\n",
       "         6.60212010e+03,  4.22906709e+03, -2.10473335e+03,\n",
       "        -1.21694947e+03,  6.50259161e+01,  1.37829027e+03,\n",
       "        -1.52530464e+03, -1.39814424e+03,  5.93168669e+02,\n",
       "        -1.02836444e+01,  4.57960925e+02,  1.75132869e+03,\n",
       "         5.73979306e+02, -7.34826226e+02,  6.06763939e+02,\n",
       "        -2.32408209e+02, -4.16070645e+02],\n",
       "       [-1.78578416e+04, -2.53854779e+04,  1.55968170e+04,\n",
       "         4.22906709e+03,  1.21241864e+04,  3.26683498e+03,\n",
       "        -6.67118820e+03,  4.58941718e+03,  2.43828028e+03,\n",
       "        -2.20323098e+03, -2.49269761e+03,  8.99833135e+02,\n",
       "         8.57449260e+02,  2.56019833e+03,  3.02538576e+03,\n",
       "         2.41459031e+03, -1.37452448e+03,  1.30342512e+03,\n",
       "        -4.12087029e+02, -6.27759893e+02],\n",
       "       [-1.76464436e+04, -1.10269392e+04,  1.14357326e+04,\n",
       "        -2.10473335e+03,  3.26683498e+03,  6.25367904e+03,\n",
       "        -2.68019007e+03,  3.04890535e+03,  7.00697601e+02,\n",
       "        -5.89708082e+02,  2.27480054e+02, -4.19091089e+02,\n",
       "         6.01145632e+02,  1.97309912e+03,  7.35238563e+02,\n",
       "         1.10468383e+03, -3.53561939e+02,  2.11278139e+02,\n",
       "         1.09945023e+02,  2.36160906e+02],\n",
       "       [ 8.82512009e+03,  1.72626343e+04, -1.05484749e+04,\n",
       "        -1.21694947e+03, -6.67118820e+03, -2.68019007e+03,\n",
       "         4.78471316e+03, -3.21385856e+03, -1.37901235e+03,\n",
       "         1.13079453e+03,  1.24161984e+03, -5.08545816e+02,\n",
       "        -6.37910901e+02, -1.52932454e+03, -1.59896836e+03,\n",
       "        -1.55365628e+03,  7.69777101e+02, -7.12460670e+02,\n",
       "         2.58023958e+02,  2.12344595e+02],\n",
       "       [-9.19332207e+03, -1.33116446e+04,  9.10297246e+03,\n",
       "         6.50259161e+01,  4.58941718e+03,  3.04890535e+03,\n",
       "        -3.21385856e+03,  3.10916542e+03,  1.05505668e+03,\n",
       "        -8.45064029e+02, -7.14156306e+02,  1.64340277e+02,\n",
       "         5.95014501e+02,  1.32603537e+03,  1.03238672e+03,\n",
       "         1.22701516e+03, -5.78026632e+02,  5.44625577e+02,\n",
       "        -1.04331747e+02, -1.05645680e+02],\n",
       "       [-8.50364261e+03, -2.28998885e+03,  1.82186170e+03,\n",
       "         1.37829027e+03,  2.43828028e+03,  7.00697601e+02,\n",
       "        -1.37901235e+03,  1.05505668e+03,  2.07537204e+03,\n",
       "        -5.17718353e+02, -3.13989713e+02, -1.29357294e+02,\n",
       "         1.74263050e+02,  4.97534542e+02,  9.59171474e+02,\n",
       "         4.29822503e+02, -2.83379317e+02,  4.09329724e+02,\n",
       "         6.88471084e+01, -1.72987451e+02],\n",
       "       [ 6.79188466e+03,  3.30515971e+03, -3.28726867e+03,\n",
       "        -1.52530464e+03, -2.20323098e+03, -5.89708082e+02,\n",
       "         1.13079453e+03, -8.45064029e+02, -5.17718353e+02,\n",
       "         1.12654151e+03,  5.53022640e+02, -1.56125509e+02,\n",
       "        -2.92041395e+02, -6.11706125e+02, -6.91900267e+02,\n",
       "        -5.52826479e+02,  3.32565560e+02, -2.30505416e+02,\n",
       "         9.03359068e+01,  6.85467165e+01],\n",
       "       [ 1.95713669e+02,  5.58914839e+03, -3.02037582e+03,\n",
       "        -1.39814424e+03, -2.49269761e+03,  2.27480054e+02,\n",
       "         1.24161984e+03, -7.14156306e+02, -3.13989713e+02,\n",
       "         5.53022640e+02,  1.38506372e+03, -2.86871759e+02,\n",
       "        -1.78621945e+01, -3.04958946e+02, -5.82551349e+02,\n",
       "        -3.65777540e+02,  3.38955166e+02, -2.27049798e+02,\n",
       "         2.40622352e+02,  3.33695968e+02],\n",
       "       [ 1.71322738e+03, -3.11868380e+03,  1.00480188e+03,\n",
       "         5.93168669e+02,  8.99833135e+02, -4.19091089e+02,\n",
       "        -5.08545816e+02,  1.64340277e+02, -1.29357294e+02,\n",
       "        -1.56125509e+02, -2.86871759e+02,  6.78887517e+02,\n",
       "         3.47692175e+00,  1.87185661e+02,  1.91796445e+02,\n",
       "         1.74431108e+02, -1.01730254e+02,  7.86512867e+01,\n",
       "        -9.67307925e+01, -1.07621059e+02],\n",
       "       [-3.08742081e+03, -1.68392752e+03,  1.59981882e+03,\n",
       "        -1.02836444e+01,  8.57449260e+02,  6.01145632e+02,\n",
       "        -6.37910901e+02,  5.95014501e+02,  1.74263050e+02,\n",
       "        -2.92041395e+02, -1.78621945e+01,  3.47692175e+00,\n",
       "         6.96462142e+02,  3.15422135e+02,  1.61750611e+02,\n",
       "         2.38153074e+02, -1.63033518e+02,  5.90128459e+01,\n",
       "        -3.90982190e+01,  1.06138292e+02],\n",
       "       [-1.01047186e+04, -6.26896329e+03,  5.83110620e+03,\n",
       "         4.57960925e+02,  2.56019833e+03,  1.97309912e+03,\n",
       "        -1.52932454e+03,  1.32603537e+03,  4.97534542e+02,\n",
       "        -6.11706125e+02, -3.04958946e+02,  1.87185661e+02,\n",
       "         3.15422135e+02,  1.60691932e+03,  7.78901919e+02,\n",
       "         7.02430170e+02, -3.49110422e+02,  1.58889368e+02,\n",
       "         4.92742023e+01, -2.42292005e+01],\n",
       "       [-7.49487354e+03, -5.27132720e+03,  3.56455725e+03,\n",
       "         1.75132869e+03,  3.02538576e+03,  7.35238563e+02,\n",
       "        -1.59896836e+03,  1.03238672e+03,  9.59171474e+02,\n",
       "        -6.91900267e+02, -5.82551349e+02,  1.91796445e+02,\n",
       "         1.61750611e+02,  7.78901919e+02,  1.45826109e+03,\n",
       "         6.37298153e+02, -3.75951530e+02,  4.26469492e+02,\n",
       "        -3.15249019e+00, -1.51992958e+02],\n",
       "       [-4.64177596e+03, -6.10281803e+03,  3.97395575e+03,\n",
       "         5.73979306e+02,  2.41459031e+03,  1.10468383e+03,\n",
       "        -1.55365628e+03,  1.22701516e+03,  4.29822503e+02,\n",
       "        -5.52826479e+02, -3.65777540e+02,  1.74431108e+02,\n",
       "         2.38153074e+02,  7.02430170e+02,  6.37298153e+02,\n",
       "         9.57976406e+02, -2.35840134e+02,  2.67223416e+02,\n",
       "        -2.45983613e+01, -4.70487158e+01],\n",
       "       [ 3.04256367e+03,  2.80838707e+03, -2.24361647e+03,\n",
       "        -7.34826226e+02, -1.37452448e+03, -3.53561939e+02,\n",
       "         7.69777101e+02, -5.78026632e+02, -2.83379317e+02,\n",
       "         3.32565560e+02,  3.38955166e+02, -1.01730254e+02,\n",
       "        -1.63033518e+02, -3.49110422e+02, -3.75951530e+02,\n",
       "        -2.35840134e+02,  5.78596607e+02, -6.97866841e+01,\n",
       "         1.23624354e+02,  9.65098807e+01],\n",
       "       [-1.34849139e+03, -2.97552306e+03,  1.21231577e+03,\n",
       "         6.06763939e+02,  1.30342512e+03,  2.11278139e+02,\n",
       "        -7.12460670e+02,  5.44625577e+02,  4.09329724e+02,\n",
       "        -2.30505416e+02, -2.27049798e+02,  7.86512867e+01,\n",
       "         5.90128459e+01,  1.58889368e+02,  4.26469492e+02,\n",
       "         2.67223416e+02, -6.97866841e+01,  5.77254179e+02,\n",
       "         1.07637894e+02, -1.03037000e+02],\n",
       "       [-5.85243118e+02,  8.41416268e+02, -5.26348767e+02,\n",
       "        -2.32408209e+02, -4.12087029e+02,  1.09945023e+02,\n",
       "         2.58023958e+02, -1.04331747e+02,  6.88471084e+01,\n",
       "         9.03359068e+01,  2.40622352e+02, -9.67307925e+01,\n",
       "        -3.90982190e+01,  4.92742023e+01, -3.15249019e+00,\n",
       "        -2.45983613e+01,  1.23624354e+02,  1.07637894e+02,\n",
       "         4.30453628e+02,  8.12351135e+01],\n",
       "       [-4.76720662e+02,  1.09302230e+03, -3.18616504e+02,\n",
       "        -4.16070645e+02, -6.27759893e+02,  2.36160906e+02,\n",
       "         2.12344595e+02, -1.05645680e+02, -1.72987451e+02,\n",
       "         6.85467165e+01,  3.33695968e+02, -1.07621059e+02,\n",
       "         1.06138292e+02, -2.42292005e+01, -1.51992958e+02,\n",
       "        -4.70487158e+01,  9.65098807e+01, -1.03037000e+02,\n",
       "         8.12351135e+01,  4.50049630e+02]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covars[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fcf5f5-274f-4d2e-a23e-e82cad3e8ec4",
   "metadata": {},
   "source": [
    "### Part 3: Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798cdd39-dee7-4191-a52a-b4f963af760d",
   "metadata": {},
   "source": [
    "In this part, we will use our trained model from part 2 to estimate the state sequence on test recordings.  You must complete the implementations of the functions below.  Again, no unit tests will be provided, so make sure to check your own work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab743ab-4abb-454a-9a2d-f6a2e7d141c1",
   "metadata": {},
   "source": [
    "The function below calculates a pairwise similarity matrix, assuming a Gaussian emission probability model.  You may use the scipy implementation of [multivariate_normal](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html) in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9daae01b-2c82-4a09-920d-248160ad58bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAFAEL \n",
    "def calcSimilarity_multivariateGaussian(O, means, covars):\n",
    "    \"\"\"\n",
    "    Calculates the matrix of likelihoods for a sequence of observations and a set of multivariate Gaussian models.\n",
    "\n",
    "    Inputs\n",
    "      - O: an D x N observation matrix, where D is the dimensionality of the feature representation and N is the number\n",
    "        of observations\n",
    "      - means: an M x D matrix specifying the distribution means, where M is the number of multivariate Gaussian models \n",
    "      - covars: an M x D x D array specifying the distribution covariance matrices, where the first index specifies the model\n",
    "        and the remaining two indices specify the model's covariance matrix\n",
    "\n",
    "    Outputs\n",
    "      - prob: a M x N matrix specifying model likelihoods, where M corresponds to the different models and where N corresponds\n",
    "        to the different observations\n",
    "    \"\"\"\n",
    "    ### INSERT CODE BELOW ###\n",
    "    \n",
    "    M = means.shape[0]  # unpack required matrix dimensions\n",
    "    N = O.shape[1]      \n",
    "\n",
    "    prob = np.zeros((M,N))\n",
    "\n",
    "    for row in range(M):  # loop through models\n",
    "        model_E = means[row, :]   # unpack current model mean\n",
    "        model_Sig = covars[row, :, :]   # unpack current model covariance\n",
    "        gauss_dist = multivariate_normal(mean = model_E, cov = model_Sig)   # generate gaussian distributiuon for current model\n",
    "        for col in range(N): # loop through audio frames\n",
    "            prob[row, col] = gauss_dist.pdf(x=O[:,col])\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        model_E = means[row, :]   # unpack current model mean\n",
    "        model_Sig = covars[row, :, :]   # unpack current model covariance\n",
    "        dist = multivariate_normal(mean = model_E, cov = model_Sig)   # generate gaussian for current model\n",
    "        print(dist.pdf(O).shape)    \n",
    "        prob[row, :] = dist.pdf(O)  # evaluate current model distribution at the observations \n",
    "        \"\"\"\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0c1aa5-0c92-434f-936d-475a0028e161",
   "metadata": {},
   "source": [
    "The function below implements the Viterbi algorithm from scratch.  Since there are lots of implementations of Viterbi online, you should not consult any direct implementations.  If you are unable to complete this part on your own, you may consult an online implementation for a grade deduction.  If you do so, please cite the resource and describe the extent of the assistance so that points may be deducted appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb6ec49-704a-46e4-be5a-cc3b3c95fb3f",
   "metadata": {},
   "source": [
    "**Graded**: Please cite any resources you consulted in implementing the function below, and the extent of the assistance: \n",
    "\n",
    "\\<PUT RESPONSE HERE\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f87d041e-f0df-419f-92c0-78ce62f9b695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAFAEL\n",
    "def viterbi(prob, A, pi):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "      - prob: a M x N matrix specifying model likelihoods, where M is the number of models and N is the number of observations\n",
    "      - A: an M x M transition probability matrix\n",
    "      - pi: a length M array specifying the initial state probability distribution\n",
    "        \n",
    "    Outputs\n",
    "      - S_est: the estimated sequence of (numeric) states\n",
    "    \"\"\"\n",
    "    ### INSERT CODE BELOW ###\n",
    "\n",
    "    M,N = prob.shape\n",
    "\n",
    "    pi = pi.reshape(M,) # make sure pi is a column vector\n",
    "    \n",
    "    #### CONSTRUCT D AND B MATRICES\n",
    "\n",
    "    D = np.zeros((M, N))  # Allocate cumulative path score matrix D\n",
    "    B = np.zeros((M, N))  # Allocate backrace matrix B\n",
    "\n",
    "    D[:,0] = np.log(pi) + np.log(prob[:,0]) # Initialize first column of D\n",
    "    \n",
    "    for f in range(1,N): # iterate over all frames (f)\n",
    "        for s in range(M): # iterate over all possible states (s) for each frame\n",
    "            transitions = A[:,s] # column vector of transitions from all states -> state s\n",
    "            p = prob[s,f] # probability of state s occuring in frame f\n",
    "            scores = D[:,f-1] + np.log(transitions) + np.log(p) # scores = scores from previous frame + transition score + probability\n",
    "            D[s, f] = np.max(scores)    # assign max of possible scores to cumulative score matrix entry\n",
    "            B[s, f] = np.argmax(scores) # assign backpointer to be index of max possible score\n",
    "\n",
    "    #### EXTRACT ESTIMATED PATH THROUGH D\n",
    "    S_est = np.zeros(N)\n",
    "    S_est[0] = np.argmax(D[:,-1]) # construct estimated state sequence in reverse order, will flip at end\n",
    "\n",
    "    for i in range(1,N):\n",
    "        S_est[i] = B[int(S_est[i-1]),N-i]\n",
    "\n",
    "    return np.flip(S_est) # flip estimated state sequence to forward orientation and return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9de3c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 2., 3.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Testing Viterbi implementation\n",
    "M = 4\n",
    "N = 5\n",
    "# construct simple prob, pi, and A matrices\n",
    "test_prob = np.zeros((M,N))\n",
    "test_prob[0,0] = 1\n",
    "test_prob[1,1] = 1\n",
    "test_prob[0,2] = 1\n",
    "test_prob[2,3] = 1\n",
    "test_prob[3,4] = 1\n",
    "test_pi = np.array([0.1, 0.1, 0.2, 0.3])\n",
    "test_A = np.zeros((M, M)) + 0.25\n",
    "print(test_prob)\n",
    "\n",
    "viterbi(test_prob, test_A, test_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03b80ff-de38-4f54-904d-9f6752f2dddf",
   "metadata": {},
   "source": [
    "Using the two functions above, estimate the state sequence for each test recording and generate the corresponding .labels file (it can have a different extension but should have the same format so as to be readable by Audacity).  Include a visualization of your estimated states alongside the audio in Audacity (as shown above).  You may use as many code cells as needed, and be sure to decompose your code appropriately!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f0523a6-49d1-419b-a78b-148305c5bddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT AS MANY CELLS AS NEEDED BELOW ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3b1b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FIRST, CREATE FUNCTION FOR ESTIMATING THE STATE SEQUENCE OF A RECORDING\n",
    "\n",
    "# this really just strings together some of the functions from above \n",
    "def estimateStateSequence(test_file, means, covars, prob, A, pi):\n",
    "    obs, hop = computeFeatures(test_file)\n",
    "    prob = calcSimilarity_multivariateGaussian(O = obs, means = means, covars = covars)\n",
    "    S_est = viterbi(prob = prob, A = A, pi = pi)\n",
    "    return S_est\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "718f7f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    These next lines extend the existing covars to the correct dimensionality and store it in TEMP. \n",
    "    TODO: switch back to using normal covars once trainHMM is fixed\n",
    "\"\"\"\n",
    "TEMP = np.zeros((6,20,20))\n",
    "for i in range(6):\n",
    "    TEMP[i,:,:] = covars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be841b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ESTIMATE STATE SEQUENCE FOR EACH TEST RECORDING\n",
    "test_files = glob.glob(\"audio/test*.mp3\") # retreive relevant files\n",
    "S_est_list = []\n",
    "\n",
    "for i in range(len(test_files)):    # iterate through files \n",
    "    S_est = estimateStateSequence(test_files[i], means=means, covars=TEMP, prob=prob, A=A, pi=pi)\n",
    "    S_est_list.append(S_est)    # append state sequence fo list (can't use np array bc different sequence lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c55f685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINE FUNCTION TO IDENTIFY STATE TRANSITIONS CORRESPONDING TO LABEL CHANGES\n",
    "def isLabelChange(state_pair):\n",
    "    \"\"\"\n",
    "        inputs: \n",
    "            state_pair = [s_i-1, s_i] (estimated states in audio frames i-1 and i of current test file)\n",
    "        returns:\n",
    "            1, if the s_i-1->s_i transition constitutes a label change\n",
    "            (   acceptable label-change transitions: \n",
    "                S->N, S->sil\n",
    "                OH->Y, OH->sil\n",
    "                sil->N, sil->Y  )\n",
    "    \"\"\"\n",
    "    s0, s1 = (int(state_pair[0]), int(state_pair[1]))\n",
    "    if states[s0] == 'S':\n",
    "        return (states[s1] == 'N' or states[s1] == 'sil')\n",
    "    elif states[s0] == 'OH':\n",
    "        return (states[s1] == 'Y' or states[s1] == 'sil')\n",
    "    elif states[s0] == 'sil':\n",
    "        return (states[s1] == 'N' or states[s1] == 'Y')\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9b3b3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMPILE LIST FOR LABEL-CHANGE INDICES OF ALL THE TEST FILES\n",
    "\n",
    "lc_inds = [] # initialize list to store label change index arrays (each entry is an array of indices for a different test file)\n",
    "for i in range(len(S_est_list)): # iterate through S_est_list entries\n",
    "    cur_inds = [0] # initialize list to store label change indices for current test file (first index = 0)\n",
    "    for index in range(1,len(S_est_list[i])):   # iterate through current state sequence\n",
    "        if isLabelChange(S_est_list[i][index-1:index+1]): # if there is a change in estimated state, log index in current index list\n",
    "            cur_inds.append(index)  \n",
    "    cur_inds.append(index) # add index for end of record\n",
    "    lc_inds.append(cur_inds)  # record temp index list (for current sequence) in compiled index list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "428b0631",
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMPILE LABEL-CHANGE TIMESTAMP LIST FROM THE LABEL-CHANGE INDEX LIST (same format)\n",
    "\n",
    "lc_ts = [] # initialize list to store label-change timestamp arrays\n",
    "for i in range(len(lc_inds)):\n",
    "    cur_ts = np.array(lc_inds[i]) * hop # convert indices to timestamps by multiplying by hop length [seconds]\n",
    "    lc_ts.append(cur_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "216886e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rburg\\AppData\\Local\\Temp\\ipykernel_10808\\2556059351.py:31: UserWarning: Non-label state sequence encountered\n",
      "  warnings.warn(message = \"Non-label state sequence encountered\")\n"
     ]
    }
   ],
   "source": [
    "### GENERATE LABEL INDICATOR SEQUENCES (yes, no, sil, ...)\n",
    "\n",
    "# first, collapse state sequence (eliminate adjacent duplicates)\n",
    "\n",
    "S_col_list = [] # initialize collapsed state estimate list\n",
    "for file_num in range(len(S_est_list)):\n",
    "    cur_S_col = [S_est_list[file_num][0]]\n",
    "    for index in range(len(S_est_list[file_num])):\n",
    "        if S_est_list[file_num][index] != S_est_list[file_num][index-1]:\n",
    "            cur_S_col.append(S_est_list[file_num][index])\n",
    "    S_col_list.append(cur_S_col)\n",
    "\n",
    "# now, generate label indicator sequence from collapsed state sequence\n",
    "\n",
    "li_list = [] # initialize label indicator list (for all test files)\n",
    "for i in range(len(S_col_list)):\n",
    "    cur_li = []\n",
    "    counter = 0\n",
    "    while (counter < len(S_col_list[i])):\n",
    "        if S_col_list[i][counter] == 0:\n",
    "            cur_li.append('sil')\n",
    "            counter = counter+1\n",
    "        elif S_col_list[i][counter:counter+2] == [4,5]:\n",
    "            cur_li.append('no') \n",
    "            counter = counter+2\n",
    "        elif S_col_list[i][counter:counter+3] == [1,2,3]:\n",
    "            cur_li.append('yes')\n",
    "            counter = counter+3\n",
    "        else: \n",
    "            counter = counter+1\n",
    "            warnings.warn(message = \"Non-label state sequence encountered\")\n",
    "        \n",
    "    li_list.append(cur_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "297f046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINE FUNCTIONS FOR WRITING TO .LABELS FILE\n",
    "\n",
    "def writeLabel(label_indicator, label_timestep, file):\n",
    "    start, stop = (label_timestep[0], label_timestep[1])\n",
    "    line = \"%f\\t%f\\t%s\\n\" % (start, stop, label_indicator)\n",
    "    file.write(line)\n",
    "\n",
    "def writeLabelFile(label_indicators, label_timesteps, file_name):\n",
    "    f = open(file_name, 'w')\n",
    "    for i in range(len(label_indicators)):\n",
    "        writeLabel(label_indicators[i], label_timesteps[i:i+2], f)\n",
    "    f.close()\n",
    "\n",
    "def generateLabelFileNames(prefix, num_trials):\n",
    "    fnames = []\n",
    "    for i in range(num_trials):\n",
    "        fname = Path(ANNOT_DIR, '%s%d.labels'%(prefix, i+1))\n",
    "        fnames.append(fname)\n",
    "    return fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec71a9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE LABELS FILES FOR TEST AUDIO FILES\n",
    "\n",
    "file_names = generateLabelFileNames(\"test\", 5)\n",
    "for i in range(len(li_list)):\n",
    "    writeLabelFile(li_list[i], lc_ts[i], file_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeba3454-c5d7-4d4f-b9dc-2919cf0bfed4",
   "metadata": {},
   "source": [
    "Comment on what you observe in your estimated state sequence, and propose some ideas on how you might improve the system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0d3d05-9e9b-4af6-b66f-dda20a0bfb9b",
   "metadata": {},
   "source": [
    "**Graded**: \n",
    "\n",
    "\\<INSERT VISUALIZATION & RESPONSE HERE\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e0a077-b593-4633-b377-45aad81fe9c7",
   "metadata": {},
   "source": [
    "### Part 4: Forced Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b966fb14-3e66-4808-806f-5ee7f6a64f87",
   "metadata": {},
   "source": [
    "In this part, you will perform forced alignment to determine the correspondence between the states in a given word-level transcription and the corresponding audio recording.  Your goal is to implement the function below, and then use it to determine the state-level alignment for train1.mp3.  Make sure to decompose your function appropriately!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0a3db6a4-c4b1-4925-b5a7-8dabd342dce1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2643174182.py, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[135], line 33\u001b[1;36m\u001b[0m\n\u001b[1;33m    Psil=\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def forcedAlignment(audiofile, word_transcript, model, stateStr2id):\n",
    "    \"\"\"\n",
    "    Performs forced alignment between a given word-level transcription and the corresponding audio recording.\n",
    "\n",
    "    Inputs\n",
    "      - audiofile: filepath to the audio recording\n",
    "      - word_transcript: a string indicating the word-level transcription.  The transcription should only\n",
    "        contain 'yes' and 'no'; the function will raise an error if it contains anything other than these two words\n",
    "      - model: tuple of (A, pi, means, covars) specifying the trained HMM\n",
    "      - stateStr2id: a dict that maps from the state string representation to its numeric identifier (e.g. stateStr2id['EH'])\n",
    "\n",
    "    Output\n",
    "      - alignment: an array specifying the coordinates of the forced alignment\n",
    "    \"\"\"\n",
    "    ### INSERT CODE BELOW ###\n",
    "\n",
    "    A,pi,means,covars=model\n",
    "    features,hopsecs=computeFeatures(audiofile)\n",
    "    feat_num, frames=features.shape\n",
    "    string_array=word_transcript.split(\" \")\n",
    "    exp_to_state,exp_str=expandWords(string_array)\n",
    "    num_exp_state=len(exp_str)\n",
    "    transition_indices=getTransitionIndices(string_array,stateStr2id) #each \n",
    "\n",
    "    lam=1 #lambda param\n",
    "    print(exp_to_state)\n",
    "    D=np.inf(np.zeros((num_exp_state,frames)))\n",
    "    B=np.zeros((num_exp_state,frames))\n",
    "\n",
    "\n",
    "    sim_mats=calcSimilarity_multivariateGaussian(features, means, covars)# provides conditional probabilities for observations and states\n",
    "\n",
    "    #initialize first column\n",
    "\n",
    "    \n",
    "    for i in range(num_exp_state):\n",
    "      p_map=exp_to_state[i] #unwrapped state index wrapped back to state space\n",
    "      conditional_p=sim_mats[p_map,0] #conditional probability for state given current state wrapped back state\n",
    "      current_t_idx=transition_indices[i]\n",
    "      A_mat_probs=A[current_t_idx[0],current_t_idx[1]]\n",
    "      D[i,0]=np.ln(A_mat_probs)+lam*np.ln(conditional_p)\n",
    "\n",
    "    for i in range(1,num_exp_state):\n",
    "       for j in range(frames):\n",
    "          if(i==0):\n",
    "             #\n",
    "          else:\n",
    "             #\n",
    "             \n",
    "             \n",
    "             \n",
    "          \n",
    "          \n",
    "    \n",
    "\n",
    "    \n",
    "    #raise NotImplementedError\n",
    "\n",
    "    #return alignment\n",
    "\n",
    "def transcriptionFromFile(filename,offset):\n",
    "    f = open(filename)\n",
    "    lines = f.readlines()\n",
    "    ret_str=lines[offset][11:-1]\n",
    "    return ret_str\n",
    "\n",
    "def expandWords(strlist):\n",
    "  expandedlist=[\"sil\"]\n",
    "  for i in range(len(strlist)):\n",
    "      if(strlist[i]==\"yes\"):\n",
    "        expandedlist.append(\"Y\")\n",
    "        expandedlist.append(\"EH\")\n",
    "        expandedlist.append(\"S\")\n",
    "        expandedlist.append(\"sil\")\n",
    "      else:\n",
    "        expandedlist.append(\"N\")\n",
    "        expandedlist.append(\"OH\")\n",
    "        expandedlist.append(\"sil\")\n",
    "\n",
    "  states, stateStr2id=getStateMapping()\n",
    "  expanded_to_state={ i:stateStr2id[expandedlist[i]] for i in range(len(expandedlist))}\n",
    "  return expanded_to_state,expandedlist \n",
    "     \n",
    "def getTransitionIndices(state_list_str,strmap):\n",
    "  indices=[]\n",
    "  for i in range(len(state_list_str)-1):\n",
    "    cur_state=state_list_str[i]\n",
    "    next_state=state_list_str[i+1]\n",
    "    indices.append([strmap[cur_state],strmap[next_state]])\n",
    "  \n",
    "  return np.array(indices)      \n",
    "\n",
    "\n",
    "states, stateStr2id = getStateMapping()\n",
    "\n",
    "\n",
    "\n",
    "t=transcriptionFromFile(Path(ANNOT_DIR,\"train.transcription\"),0)\n",
    "forcedAlignment(Path(AUDIO_DIR,\"test1.mp3\"),t,(A, pi, means, covars),stateStr2id)\n",
    "print(stateStr2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3ab7c3-3365-4811-84a7-6b21654e5f56",
   "metadata": {},
   "source": [
    "Once you have implemented the forced alignment function above, use it to estimate the state-level alignment for train1.mp3.  Visualize the predicted alignment alongside the audio in Audacity, and also include the word-level alignment from part 1 (that you manually created).  Comment on how the forced alignmend method improves the quality of the alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "342c62a1-f790-4209-bc1b-3bc75b0cda24",
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT AS MANY CELLS AS NEEDED BELOW ###\n",
    "alignment=forcedAlignment()\n",
    "\n",
    "audiofile = Path(AUDIO_DIR, \"train1.mp3\")\n",
    "labelfile = Path(ANNOT_DIR, \"train1.labels\")\n",
    "\n",
    "train1_obs, hop = computeFeatures(audiofile = audiofile) # compute observations and hop size from audio file\n",
    "\n",
    "states, stateStr2id = getStateMapping() # retrieve list of possible states and mapping between string and integer state representation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33abd41-80f9-441e-a1c8-2f98de6397c2",
   "metadata": {},
   "source": [
    "**Graded**: Include the visualization below and comment on what you observe.\n",
    "\n",
    "\\[Show predicted alignment in Audacity\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458f0c77-ca26-408d-ab0d-38eace106eb6",
   "metadata": {},
   "source": [
    "### Part 5: Retrain Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8868303-cf1a-44d1-91d9-da0cf2ee2baf",
   "metadata": {},
   "source": [
    "In the last part of the assignment, you will use your initial model from part 2, perform forced alignment to generate .forcealign files for all weakly labeled training data, re-train your HMM, and then perform inference on the test data with the new model.  Provide a snapshot in Audacity comparing the predictions from part 3 and part 5 on a single test file.  Comment on any differences you observe, what the re-trained model is doing well, and where the re-trained model is making errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "626c7be8-0316-4b3a-8e2e-f61c738ae025",
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT AS MANY CELLS AS NEEDED BELOW ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d617af9-85e1-451e-ba5a-75e06e869886",
   "metadata": {},
   "source": [
    "**Graded**:  \n",
    "\\[INSERT VISUALIZATION & COMMENTS HERE\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a68883-689e-42c5-8f7b-279b1bcae712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "E207_Spr24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
